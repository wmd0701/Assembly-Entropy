\documentclass[11pt]{article}


\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}


\title{{\small LEHRSTUHL F\"UR RECHNERTECHNIK UND RECHNERORGANISATION}\\Aspekte der systemnahen Programmierungbei der Spieleentwicklung}
\author{Nassim Boukadida ,Mohamed Taieb Ben Haddej , Mengdi Wang}
\begin{document}
\maketitle

\tableofcontents
\newpage
\section{Einleitung} 





%{\Huge I)Einleitung:}\\\\
In der Vorliegenden Arbeit besch\"aftigen wir  uns mit dem Thema der Informationstheorie , ins besonders mit der Entropie , ein Mass f\"ur den mittleren Informationsgehalt einer Nachricht.Durch den zwei pr\"asentierenten Bereiche "theoretisch" und "praktisch" wird man genau verstehen , nicht nur die Konzeption der matimatische Formel der Entropie , sowie auch wie man ein Algorithmus f\"ur diese Formel im Rasberry Pi 3 implementieren kann. \\
Bei dem theoretischen Bereich haben wir untersucht , wie kann man den Logarithmus zur Basis Zwei mit einer Reihungdarstellung  in guter N\"aherung berechnen kann . Zudem haben wir auch die Calling Convention im Kontext auf den Fliesskommazahlen in Arm und die Entwicklung ihres Speicherstruktures  gut detailliert studiert und erkl\"art . Auf der anderen Seite haben wir im Praktischen Bereich eine Algorithmus  implementiert , welche die Entropy eines Array von Single Precision berechnet und zur\"uckgibt , wobei das Input der Daten mit verschiedenen M\"oglichkeiten eingegeben wird. \\  
\section{Aufgabestellung} 
{\color{red} Theoretischer Teil :} \\
 1)wann gilt die Gleichheit in $H(X)<= log_2{(|X|)}$?\\
 2)Eigenschaften von \{P\} f\"ur eine Wahrscheinlichkeitsverteilung.\\
3)Eine mathematische Approximation zu log2 (Reihendarstellung oder Tabellen-Lookup).\\
4)Selbst lernen von Calling-Conventions im Kontext von ARM und GCC f\"ur float.\\
5)Performanz von Implementierung zu pr\"ufen und mit C-Implementierung zu vergleichen.\\\\
{\color{red} Praktischer Teil :}\\
1)Eingabe: eine Datei mit unbestimmter Anzahl von float.\\
2)Rahmenprogramm in C: konvertieren die Eingabedatei in Array und einen Pointer darauf anbieten.\\
3)float calc\_entropy(float* data, unsigned int len):\\
3.1 \"Uberpr\"ufen Inputvalidit\"at.\\ 
3.2 berechnen die Entropie vom Array data, mit L\"ange len  (Achtung: nur 4 Grundrechenarten erlaubt).\\\\
{\color{red} Bonus Aufgabe :}\\
1)Generieren eine große Anzahl von Zufallsvariablen mit rand() und berechnen die  Entropie davon, vergleichen das erwartete Ergebnis mit erhaltenes \\
2)calc\_entropy\_approx(float* data, unsigned int len):  Kein log2 berechnen, sondern mit Shifts zu approximieren  Pr\"ufen die Genauigkeit im Vergleich zu calc\_entropy und stellen das Ergebnis in Form von Diagramm.\\\\

\section{Theoretischer Teil:}
\begin{Large}
%\Huge{II)Theoretischer Teil:}\\  
\end{Large}
\subsection{Eine Wahrscheinlichkeitsverteilung:}
\begin{large}
%{\Large 1)Eine Wahrscheinlichkeitsverteilung: }\\

Damit eine Menge von Zahlen eine Wahrscheinlichkeitsverteilung sein kann, muss diese Menge Zahlen zwischen $(0,1]$ enthalten und die Summe dieser Zahlen muss 1 sein. \\
\end{large} 

\subsection{G\"ultigkeit von der Gleichung $H(X) <= log_2(|X|)$:}
  L\"osung: wenn f\"ur alle beliebigen x1 und x2 in X gilt: P(X = x1) = P(X = x2)\\
 Beweis: \\
 angenommen, $|X| = n ==>$ f\"ur alle $x \in X$: $P(X = x) = \frac{1}{n} $\\
 $ H(X) = - \sum\limits_{x \in X}P(X = x) * log_2{(P(X = x))}$     \\    
  $= n * (-1) * 1/n * log_2{\frac{1}{n}} \\        
   = -1 * log_2{(1/n)}\\
  = log_2{n} \\
   = log_2{(|X|)} \\
$
\begin{large}
\subsection{N\"aherung vom Logarithmus Basis 2:}
%{\Large 2)N\"aherung vom Logarithmus Basis 2: }\\

Wir wissen von der Analysis Vorlesung dass:
 $$ ln{(1+x)}=\sum\limits_{v=1}^n\frac{(-1)^{v+1}}{v}*x^v  \ \ mit \ \ x \in (-1,1] $$   \\ Gleichbeteudend\\
 $$ ln{(1-x)}= -\sum\limits_{v=1}^n\frac{x^v}{v}  \ \ mit \ \ x \in [-1,1) $$ \\
 das heisst, wir k\"onnen nur $ln{(x)}$ mit $x \in (0,2]$ berechnen , und das gen\"ugt , da wir nur $ln{}$ von Wahrscheinlichkeiten , die in [0,1] liegen , berechnen werden. Nachdem wir $ln{(w)}$ berechnen, wir k\"onnen $Log_2(w)$ bestimmen, indem wir einen Basiswechsel machen.Durch: $$\displaystyle{log_2{x}=\frac{ln{w}}{ln{2}}}$$\\
\end{large} 
\begin{large}
\subsection{Speicherstruktur f\"ur Fliesskommazahlen:}
%{\Large 3)Speicherstruktur f\"ur Fliesskommazahlen:}\\

Nach dem Recherchieren \"uber dieses Thema, haben wir schon einige Lernstoffe gefunden. (z.B. https://www.h-schmidt.net/FloatConverter/IEEE754.html) In C und in Assembler, eine Fliesskommazahl (float) hat 32 Bits. Beim Speichern von einem float wird das float zuerst in Bin\"arsystem in Form von Produkt von 1.abcdef… und 2er Exponent verwandelt. Zum Beispiel:\\\\
0.5 in Dezimalsystem $ = 1.0 * 2^{-1}$\\
16.75 in Dezimalsystem $ = 1.000011 * 2^{4}$\\
daher kann man die Form so darstellen:\\
 \centerline{1.abcdef....$*2^n$}\\\\
 Die Ziffer vom Komma ist immer 1, und die Ziffern a, b, c, d, e, usw.. sind entweder 0 oder 1 in Bin\"arsystem, und n ist das 2er Exponent. Beim Speichern von float wird genau diese Form verwenden. Man teilt dann 32 Bits so auf:\\\\
 \centerline{1 Bit f\"ur Flag (0 f\"ur positiv, 1 f\"ur negativ)} \\ 
\centerline{8 Bits f\"ur Exponent (reales Exponent $+ 127$)}\\
 \centerline{23 Bits f\"ur Mantisse (die Ziffer 1 vom Komma ist nicht eingeschlossen)}\\\\
Man muss hier auf zwei Punkte achten: \\
{\Large 1.}  Nicht das echte Exponent, sondern das echte $ Exponent + 127$ wird gespeichert. Zum Beispiel, f\"ur 0.5 in Dezimalsystem ist $das \  Exponent-1$, so hier wird $-1 + 127 = 126$ gespeichert. Analog dazu wird $4 + 127 = 131$ gespeichert.\\
{\Large 2.} Die Ziffern 1 vor dem Komma wird nicht gespeichert.Da f\"ur alle Zahlen steht immer 1 vor dem Komma,braucht man nicht die Ziffer zu speichern, so kann man mit 23 Bits aber 24 Bits von Mantissen behandeln. Zum Beispiel,f\"ur 0.5 ist die zu speichernden Mantisse 0000……(23 mal 0).Analog dazu ist die Mantisse f\"ur 16.75 so: 0000110000….($23 - 6 = 17$ mal 0 nach 1111).\\
Hier w\"urden wir aber ein totales Beispiel einf\"uhren: das Speichern von float Wert 8.5:\\\\
\centerline{$8.5 = 1.0001 * 2^3$}\\
Exponent: $3 + 127 = 130$, in Bin\"arsystem $130 = 1000 0010$ \\
Mantisse: 00010000….($23 - 4 = 19$ mal 0 nach 1), in Dezimalsystem $=524288$\\\\
So 8.5 wird so gespeichert:\\
\centerline{ {\color{blue}0 }	{\color{red}1000 0010 }	0001 0000000000000000000} \\\\
{\color{blue}$->$ 1 Bit f\"ur Flag(0 , so der Wert ist positiv)}\\
{\color{red}$->$ 8 Bits f\"ur Exponent($3+127=130$(dezimal)=1000 0010(binaer))}\\
$->$ 23 Bits f\"ur Mantisse \\
Das ist aber auch der Grund daf\"ur, warum man sofort den Befehl vcvt aufrufen muss, wenn man ein Wert aus ARM Register in FPU Register mit dem Befehl vmov kopiert. Der Befehl vmov kopiert nicht den Wert, sondern alle Bits. Aus demselben Grund werden alle Bits von einem FPU Register nach ARM Register kopiert, wenn man z.B. vmov r0, s0 aufruft. Das bedeutet aber, wenn s0 beispielsweise 8.5f speichert, sind die Bits von r0 nach Aufruf von vmov r0, s0 so:\\\\
\centerline{0	1000 0010	0001 0000000000000000000}\\\\
Dieser Wert ist nat\"urlich nicht 8 (8.5f nach unten abgerundet), sondern 1091043328 in Dezimalsystem.\\
Das Exponent von einem float f (f\"ur Werte kleiner als 1.0f ist es Exponent + 1) ist aber gerade der Integer-Teil von dem Ergebnis von $log_2{f}$. Zum Beispiel:\\\\
\centerline{$log_2{64.8}=6.0179$,integer-Teil ist 6, und $64.8 = 1.00000011 * 2^6$ }\\
\centerline{$log_2{0.0047}=-7.0589$,integer-Teil ist -7, und $0.0075 = 1.11101 * 2^{-8}$ }\\\\
Dann ist der Integer-Teil von $log_2{X}$ sehr einfach zu erhalten: man soll zuerst X von FPU Register nach ARM Register kopieren, dann die Bits im ARM Register nach rechts um 23 Stellen schieben. Da wir bekommen immer positive Eingaben, ist das erste Bit immer 0. Das bedeutet, dass der Wert mit Shifts nach rechts um 23 Stellen gerade der Integer-Teil ist. \\\\






\end{large} 

 




\section{Praktischer Teil:}
%{\Huge III)Praktischer Teil: }\\\\
\subsection{Benutzer Dokumentation:}
%{\LARGE 1)Benutzer Dokumentation:}\\

Wenn man das Programm benutzen m\"ochte, braucht man zuerst den Befehl {\color{red}make} aufrufen. Dann wird eine kompilierte Datei namens „entropie“ (ohne Suffix) erzeugt. Man kann jetzt {\color{red}./entropie} aufrufen, um das Programm auszuf\"uhren.
Es wird sofort danach gefragt, welches Array man zum Testen verwenden m\"ochte. Es gibt hier drei Wahlm\"oglichkeiten:\\ \\\centerline{{\color{red}„default array“}, {\color{red}„input file“} und {\color{red}„large random array“}.}  \\\\
{\color{red}„default array“}: das Array zum Debuggen und Testen w\"ahrend der Implementierung. Dieses  
                             Array hat 16 Elemente: float test[] = {0.05, 0.17, 0.07, 0.01, 0.2, 0.005, 0.13,  
                             0.065, 0.07, 0.08, 0.025, 0.025, 0.006, 0.004, 0.055, 0.035}; \\\\
{\color{red}„input file“}: Anforderung von praktischer Aufgabe. Man kann dazu eine Datei mit  
                      Fliesskommazahlen schreiben, und diese Datei als Eingabearray verwenden. \\\\
{\color{red}„large random array“}: Anforderung von Bonusaufgabe. Hierbei wird zuerst ein grosses Array  
                                       (mit 30000 Elementen) von Zufallsvariablen durch rand() generiert.  
                                        Danach werden die Wahrscheinlichkeiten von den Zufallsvariablen  
                                        berechnet und als Eingabe verwenden. \\\\
\centerline{\includegraphics[scale=0.8]{1}}
Achtung:\\
{\LARGE 1.}	Wenn man hier „input file“ w\"ahlt, muss man danach den Namen von der Datei eingeben. Zum einfachen Testen steht eine Datei namens „input“ mit 9 Elementen zur Verf\"ugung: 0.1, 0.07, 0.13, 0.22, 0.08, 0.15, 0.15, 0.05, 0.05\\
{\LARGE 2.}	Wenn man hier „large random array“ w\"ahlt, muss man aber zig Sekunden lang warten, da das Generieren von Eingabearray dauert.\\\\
Es wird dann die L\"ange von dem Eingabearray ausgegeben und eventuell danach gefragt, ob Sie das Array modifizieren m\"ochten.\\\\
\centerline{\includegraphics[scale=0.8]{2}}\\
Wenn die Summe von allen Elementen in dem Array nicht 1.0f ist, wird es danach gefragt, ob Sie das Array modifizieren m\"ochten. Auch wenn Sie alle Elemente schon manuell aufsummieren und als Ergebnis 1.0 bekommen, k\"onnte es auch vorkommen, dass die Summe im Programm noch nicht 1.0f ist. Der Grund daf\"ur besteht darin, dass das Speichern von Fliesskommazahlen nicht voll pr\"azis ist. Manchmal beim Speichern werden die float Werte gerundet (meistens nach unten abgerundet), sodass die Summe nicht mehr 1.0f ist. Wenn Sie hier 1 eingeben, wird das letzte Element vom Array ein bisschen ver\"andert, sodass die Summe wiederum 1.0f betr\"agt. Wenn Sie aber andere Zahl eingeben, werden Sie als Output von allen calc\_entropy\_* Methoden -1.0f bekommen.\\\\
Danach wird es danach gefragt, wie viele mal man das Ausf\"uhren wiederholen m\"ochte. \\\\
\centerline{\includegraphics[scale=0.8]{3}}\\
Geben Sie bitte eine grosse Zahl ein (z.B. 50000 oder 130000), sodass man die Laufzeiten besser vergleichen kann. Und nat\"urlich, je gr\"osser die Wiederholungsanzahl ist, muss man desto l\"anger auf Output warten. Und insbesondere, bitte vermeiden Sie grosse Zahl einzugeben, wenn Sie „large random array“ ausw\"ahlt, sonst k\"onnen Sie vielleicht zig Minuten lang warten.\\\\
Normalerweise nach 1 bis 2 Minuten k\"onnen Sie schon ein Output bekommen. Wenn Sie schon viele Minuten lang gewartet und keine R\"uckkopplung erhalten, sind Sie h\"ochstwahrscheinlich von Raspberry Pi Cluster oder anderem Server getrennt. Bitte verbinden Sie sich nochmal.\\\\
Zum Lesen von Output: Das Output sieht so aus:\\\\
\centerline{\includegraphics[scale=0.8]{4}}\\
Result in calc\_entropy: das Ergebnis von calc\_entropy, wobei eine Reihendarstellung von $log_2{X}$ in Assembler implementiert wird und SIMD-Befehle verwendet werden\\\\
Result in calc\_entropy\_approx: das Ergebnis von calc\_entropy\_approx, Bonusaufgabe, wobei eine Approximation zu $log_2{X}$  mithilfe von Shifts implementiert wird\\\\
Result in calc\_entropy\_old, das Ergebnis von calc\_entropy\_old, wobei eine Reihendarstellung von $log_2{X}$  in Assembler implementiert wird, aber ohne SIMD-Befehle\\\\
Result in calc\_entropy\_c1: das Ergebnis von calc\_entropy\_c1, wobei Bibliothek-Funktionen in C f\"ur $log_2{X}$  verwendet wird. Dieses Ergebnis dient zum Standardergebnis\\\\
Result in calc\_entropy\_c2: das Ergebnis von calc\_entropy\_c2, wobei eine Reihendarstellung von $log_2{X}$  in C implementiert wird\\\\
Elapsed time for implementation with simd instructions: die Laufzeit von calc\_entropy bez\"uglich Wiederholungsanzahl\\\\
Elapsed time for implementation without simd instructions: die Laufzeit von calc\_entropy\_old bz\"uglich Wiederholungsanzahl\\\\
Elapsed time for implementation in C: die Laufzeit von calc\_entropy\_c2 bez\"uglich Wiederholungsanzahl\\\\
Bitte achten Sie hier auf folgende:\\\\
{\LARGE 1.}	Wenn die Summe von Elementen im Array nicht 1.0f ist, werden Sie hier als Ergebnisse von allen calc\_entropy\_* Methoden -1.0f erhalten. \\\\
\centerline{\includegraphics[scale=0.8]{5}}\\\\
{\LARGE 2.}	calc\_entropy, calc\_entropy\_old, calc\_entropy\_c2 sollen auf jeden Fall ganz gleiche oder fast gleiche Ergebnisse haben, da sie implementieren dieselbe Reihendarstellung.\\\\
{\LARGE 3.}	Die Laufzeit von calc\_entropy soll k\"urzer als die von calc\_entropy\_old und calc\_entropy\_c2 sein, da calc\_entropy hat SIMD-Optimierung.\\\\
{\LARGE 4.}	Die Laufzeit von calc\_entropy\_old soll in der N\"ahe von der von calc\_entropy\_c2 sein.\\\\\\

\subsection{Entwickler Dokumentation:}
%{\LARGE 2)Implementation allgemeine Dokumentation:}\\
\centerline{\includegraphics[scale=0.8]{Ftable}}
\centerline{\includegraphics[scale=0.9]{Dtable}}


{\Large $->$  C-Implementierung:}\\\\
{\color{red}{\Large float calc\_entropy\_c1(float *data, unsigned int len):}}\\\\
Diese Methode berechnet Entropie vom Array mit L\"ange len mithilfe von C Bibliothek Funktion log(). Der Pseudo-Code sieht so aus:\\
Loop for elements e in array:\\
	sum += e;\\
test if sum == 1.0f;\\
Loop for elements e in array;\\
	entropy += e * log(e);\\
entropy = entropy / log(2.0f);\\\\
{\color{red}{\Large float calc\_entropy\_c2(float *data, unsigned int len):} }\\\\
Diese Methode implementiert eine Reihendarstellung von ln(1-x), um log2x zu berechnen. Der Pseudo-Code daf\"ur sieht so aus:
Loop for elements e in array:\\
	sum += e; \\
test if sum == 1.0f;\\
Loop for elements e in array {	\\	
X = 1 – e;\\
expX = 1;\\
sum = 0;\\
(*)Loop for j: 1~500{\\
	expX *= X\\
	sum -= expX / j;\\
}\\
entropy -= e * sum;\\
}\\
calculate ln0.5 with (*)\\
ln2 = -ln0.5;\\
entropy /= ln2\\\\
{\color{red}{\Large float* CreateProbabilityArray()}}\\
Diese Methode gibt einen Zeiger auf das Array von Wahrscheinlichkeiten zur\"uck.
Vorgehensweise:\\\\
Wir erzeugen ein Array arr1 mit einem großen Length ( 30000 in unserer Implementierung) und wir erf\"ullen es mit zuff\"aligen Zahlen ( zwischen 0 und RAND\_MAX ).\\\\
{\color{red}{\Large  void sortArray(int *arr, int ls) }}\\\\
Diese Methode sortiert das Array arr.\\\\
{\color{red}{\Large  numberOfDifferentElements(arr1, length) :}}\\\\
Diese Methode gibt das Length nbe zur\"uck,das die Anzahl von den Elementen entspricht, die verschieden voneinander sind.
Wir erzeugen nun ein zweites Array arr2 mit dem Length nbe, \\\\
{\color{red}{\Large  arrayWithoutRepetition(arr1, arr2, length, nbe)}}\\\\
Diese methode legt die Elementen von arr1 in arr2 ohne Wiederholungen.\\
Wir erzeugen danach ein drittes Array arr3 mit dem Length nbe, und die Methode \\\\
{\color{red}{\Large  arrayWithNumberOfEelemts(arr3, arr2, arr1, nbe, length): }}\\\\
 Diese Methode setzt in jedes Field von arr3 die Anzahl von Wiederholungen vom \"ubereinstimmenden Element von arr2 in arr1.\\
Wir erzeugen endlich ein Array tar mit dem selben Length nbe, und unsere Methode\\ \\
{\color{red}{\Large  calcProbability(arr3, tar, nbe, length):}}\\\\
Diese Methode teilt jedes Element von arr3 durch 30000. (30000 ist das Length, das wir am Anfang gew\"ahlt haben).\\\\
{\color{red}{\Large  Bemerkung:}}\\\\
{\color{red} void AdjustArray(tar, arrayLength)} wird versichern, dass die Summe von den Elementen von tar 1 ist. (weil die Summe manchmal bei 0.99 liegt ). Damit wir dieses kleine Problem l\"osen, addieren wir die Differenz zwischen 1 und allen Elementen zu dem letzten Element.\\\\
{\color{red}{\Large float* fileToArray(int* len):}} \\\\
Diese Methode konvertiert eine Datei in Array. Als R\"uckgabe wird ein Pointer auf das Array zur\"uckgeliefert, das durch malloc() und realloc() erzeugt wird, deshalb muss man am Ende von dem Programm free() f\"ur den Pointer aufrufen. Die L\"ange von dem Array wird auf len zugewiesen.\\
Struktur zu dieser Methode ist relativ einfach, man kann daher einfach einen Blick auf Implementierung und Kommentare werfen.\\\\
{\color{red}{\Large void main():}}\\\\
Das ist die main-Methode von dem Programm. Hierbei soll man beim Ausf\"uhren verschiedene Eingaben ausw\"ahlen, um auf Performanz und Genauigkeit zu pr\"ufen. Struktur zu dieser Methode ist relativ einfach, man kann daher einfach einen Blick auf Implementierung und Kommentare werfen.\\\\
{\Large $->$ Assembler-Implementierung:}\\\\
{\color{red}{\Large Makefile:}}\\\\
CFLAGS=-mfpu=neon -mfloat-abi=hard -lm -O3 \\
all: entropie \\
entropie: entropie.c entropie.S curtime.c RandomInput.c\\
Achtung: Weil wir mathematische Funktion log() in C Bibliothek benutzt haben, m\"ussen wir hier -lm als Kompilierparameter \"ubergeben. Dar\"uber hinaus um die Methode mit SIMD-Befehlen besser zu kompilieren und auf Performanz besser zu pr\"ufen, muss man als Optimierungsstufe h\"oher als -O2 \"ubergeben (hier O3).\\\\
{\color{red}{\Large calc\_entropy(float* data, unsigned int len):}} \\\\
Diese Methode ist unsere L\"osung f\"ur praktische Aufgabe. Sie implementiert eine Reihendarstellung zu ln(1-x) und damit berechnet log2 mithilfe von Basiswechsel der Logarithmen. In dieser Methode ist eine Optimierung mit SIMD-Befehlen schon verwendet. (Die alte Methode ohne SIMD-Optimierung, die wir zuerst implementiert haben, wird jetzt in calc\_entropy\_old umbenannt).
Die Struktur und der Pseudo-Code f\"ur calc\_entropy sehen so aus:\\\\
calc\_entropy:\\
	save return address; (push {lr})\\
	test input validity; (branch to input\_valid)\\\\
	Entropys = 0 ;
loop1:\\
	check amount of remaining elements in array;\\
	less than 4: branch to end1\\
	larger or equal to 4: branch to simd\_ln1\_X (tips: the symbol – is not allowed)\\
	branch to loop1\\\\
simd\_ln1\_X:\\
	expXs = 1;\\
	load 4 elements Es from array;\\
	Xs = 1 – Es;\\
	Sums = 0;\\\\
simd\_ln1\_X\_loop:\\
	Loop for n: 1-500{\\
		$n > 500$: branch to simd\_ln1\_X\_end\\
		calculate 1 / n;\\
		expXs *= Xs;\\
		calculate expXs*(1/n); 
		Sums -= expXs*(1/n);\\
\}\\\\
simd\_ln1\_X\_end:\\
	calculate Es * Sums; (e * lne)\\
	Entropys -= Es * Sums;\\
	branch back (bx lr);\\\\
end1:\\
	sum up Entropys vector into entropy;\\\\
loop\_lessthan4:\\
	check amount of left elements in array;\\
	less or equal to 0: branch to end\_lessthan4;\\
	load one element e from array;\\
	calculate lne; (branch to ln1\_X);\\
	calculate $e * \ln{e}$;\\
	$entropy -= e * \ln{e}$;\\
	branch to loop\_lessthan4;\\\\
end\_lessthan4:\\
	calculate $\ln{0.5}$;\\
	$\ln{2} = -\ln{0.5}$;\\
	entropy /= ln2;\\
	branch back; (pop {pc})\\\\
{\color{red}{\Large ln1\_X:}}\\\\
Diese Methode ist eine innere Funktions in entropie.S. Sie berechnet ln() von Input. Beim Aufrufen von ln1\_X soll man Input in s0 speichern, und nach dem Ausf\"uhren wird das Ergebnis auch in s0 gespeichert.
Der Pseudo-Code daf\"ur sieht so aus:\\\\
ln1\_X:
	X = 1 – input;\\
	sum = 0;\\
	expX = 1;\\\\
loop2:\\
	loop for n: 1 - 500{\\
		$n > 500$: branch to end2;\\
		expX *= X;\\
		calculate expX / n;\\
		sum -= expX / n;\\
\}\\\\
end2:\\
	branch back; (bx lr)\\\\
{\color{red}{\Large calc\_entropy\_approx(float* data, unsigned int len)}}:\\\\
Diese Methode ist unsere L\"osung f\"ur zweite Bonusaufgabe. Hierbei wird log2x nicht explizit berechnet, sondern mithilfe von Shifts approximiert. \\
Der Pseudo-Code daf\"ur sieht so aus:\\\\
calc\_entropy\_approx:\\
	save return address; (push {lr})\\
	test input validity; (branch to input\_valid)\\
	sum = 0;\\
loop3:\\
	loop for elements in array\{\\
		out of array boundary: branch to end3;\\
		load an element e from array;\\
		branch to log2\_shift; (calculate log2e)\\
		calculate e * $\log_2{e}$;\\
		$sum -= e * \log_2{e}$;\\
\}\\\\
end3:\\
	branch back; (pop {pc})\\\\
log2\_shift:\\
	get mantissa of e;\\
	get exponent of e;\\
	branch to interpolate;\\\\
interpolate:\\
	check if mantissa of e equals to 0;\\
	yes:$ log_2{e}$ = exponent of e, just branch back (bx lr);\\
	interpolate between exponent of e and 1 + exponent of e; (you may need to have a 
              look at Ausarbeitung to understand how it works)\\
	$log_2{e}$ = exponent of e + interpolation result;\\
	branch back; (bx lr)\\\\
{\color{red}{\Large calc\_entropy\_old(float* data, unsigned int len):}}\\\\
Diese Methode ist die Basis zu calc\_entropy. Sie implementiert eine Reihendarstellung zu ln(1-x) und dann berechnet log2 mithilfe von Basiswechsel und liefert am Ende Entropie zur\"uck.\\
Der Pseudo-Code daf\"ur sieht so aus:\\\\
calc\_entropy\_old:
	save return address; (push {lr})\\
	test input validity; (branch to input\_valid)\\
	calculate ln0.5;\\
	ln2 = -ln0.5;\\
	sum = 0;\\\\
loop\_old:\\
	loop for elements in array{\\
		out of array boundary: branch to end\_old;\\
		load an element e from array;\\
		calculate lne; (branch to ln1\_X)\\
		calculate e * lne:\\
		sum -= e * lne;\\
\}\\\\
end\_old:\\
	sum /= ln2;\\
	branch back; (pop {pc})\\\\
\color{red} {\Large Input\_valid:}}\\\\
Diese Methode \"uberpr\"uft die Eingabevalidit\"at von dem Array (Alle Elemente zwischen 0 und 1, und Summe ist gleich 1). Die soll man am Anfang von allen calc\_entropy\_* Methodef aufrufen. Daf\"ur braucht man kein weiteres Parameter, nur r0 f\"ur float* data, und r1 f\"ur unsigned int len.\\
Der Pseudo-Code daf\"ur sieht so aus:\\\\
input\_valid:\\
	sum = 0;\\\\
valid\_loop:\\
	loop for elements in array{\\
		out of array boundary: branch to valid\_loop\_end;
		load one element e from array;\\
		$e <= 0$: branch to invalid;\\
		$e > 1$ : branch to invalid;\\
		sum += e;\\
\}\\\\
valid\_loop\_end:\\
	sum != 1: branch to invalid;\\
	branch back; (bx lr)\\\\
invalid:\\
	set return value to -1.0f; (for all calc\_entropy\_* methods)\\
	branch back; (pop {pc}) \\



\subsection{Entropie ausf\"uhrliche Implementierung:}
%{\LARGE 3)Entropie ausf\"uhrliche Implementierung:}\\
( F\"ur mehr Details werfen Sie einen Blick auf die Entwicklersdokumentation und Implementierung )\\\\
Wir schauen zuerst mal die Methode calc\_entropy(float *data, unsigned int len) ohne SIMD-Befehle. Diese Methode macht folgende:\\\\
{\Large 1.} \"uberpr\"ufen, ob die Werte des Eingabearrays (data, mit Laenge len) die Eigenschaften einer Wahrscheinlichkeitsverteilung erf\"ullen. Das bedeutet,f\"ur alle $i < len$ soll es gelten:  $$0<data[i]<=1.0f \  und \  \sum_{i=0}^{n-1} data[i] = 1.0f$$\\
{\Large 2.} berechnen die Entropie von dem Array und liefert das Ergebnis zur\"uck. \\
Dar\"uber hinaus gibt es zu der Implementierung von dieser Methode noch zwei Anforderungen:\\\\
{\Large 1.}	Man muss den Logarithmus zur Basis zwei ($log_2{X}$) in guter N\"aherung mit Hilfe entweder von einer Reihendarstellung oder von einem Tabellen-Lookup berechnen\\\\
{\Large 2.} Nur vier Grundrechenarten sind erlaubt ($+, -, *, /$)\\\\
Wir fangen die Implementierung mit \"Uberpr\"ufung auf Inputvalidit\"at an. Wir m\"ussen f\"ur alle Elemente data[i] in dem Array pr\"ufen, ob $0 < data[i] ≤ 1.0f$ ist. Weiter brauchen wir auch alle data[i] aufsummieren und pr\"ufen, ob die Summe gleich 1.0f ist. Weil es hier sich immer um Fliesskommazahlen (FPU Register) behandelt, kann man hier den Befehl cmp nicht benutzen, was nur f\"ur Integer (ARM Register) funktioniert. So man braucht hier einen anderen Befehl: vcmp ({\color{red} Schwerpunkt })\\\\
{\color{red} Schwerpunkt:vcmp }\\\\
Die Grammatik f\"ur vcmp bei unserem Projekt sieht so aus:\\\\
\centerline{vcmp.f32 s1, s2	oder vcmp.f32 s1, \#0}\\\\
Man soll hier darauf achten, dass vcmp direkt f\"ur Zahlen funktionieren kann, aber diese Zahl darf nur 0 (\#0) sein. Weiter ist es ganz entscheidend, dass im Vergleich zum Befehl cmp, vcmp keine Flags in APSR (ARM Statusregister) setzt, sondern in FPSCR (FPU Statusregister). Das heisst aber, das Nutzen von allen Suffixen nach vcmp, die von Vergleichsergebnis abh\"angig sind (z.B. le, gt, ne, usw.), wird zum logischen Fehler f\"uhren. Um dieses Problem zu loesen, braucht man einen anderen Befehl: \\\\
\centerline{VMRS APSR\_nzcv, FPSCR} \\\\
Der Befehl vmrs transportiert den Inhalt in einem FPU Register in ein ARM Register. In diesem Fall werden die Flags von FPSCR an APSR transportiert. Dann kann man Suffixe reibungslos benutzen.\\
Nach dem \"Uberpr\"ufen auf Inputvalidit\"at implementieren wir dann die Berechnung von Entropie. Daher ist es am wichtigsten, eine Reihendarstellung oder in Tabellen-Lookup mit nur vier Grundrechenarten f\"ur $log_2{X}$ zu finden.\\
Jetzt fangen wir an, calc\_entropy zu implementieren. Der ganze Code l\"asst sich in vier Schritte aufteilen:\\\\
{\Large 1.}	Implementierung von der Schleife zum Berechnen f\"ur $ln{X}$:\\ \\
{ \color{blue} $x = 1.0f - input; \\
exp\_x = 1.0f; \\
Loop \ for\ i \in [1, 500]: $\\
\{ \\ 
        $exp\_x *= x; $\\
        $result -= exp\_x / i; $\\
\} } \\\\
{\Large 2.} Berechnen ln2 ( {\color{red} Trick 1:Statt $ln{2}$ zu berechnen, berechnen wir eigentlich $ln{0.5}$, und $ln{0.5} = ln{\frac{1}{2}} = -ln{2}$. Wir machen das so weil wir schon gefunden haben, dass mit unserem Algorithmus ln0.5 pr\"aziser als $ln{2}$ ist.}) : \\\\
{\Large 3.} Implementierung von der Schleife zum Berechnen f\"ur Entropie:\\\\
{\color{blue} $entropy = 0$;\\
Loop \ for \ i $\in$ [0, len - 1]: \\
\{ \\
        $entropy -= data[i] * ln(data[i])$; \\
\}
}\\\\
{\Large 4.} Entropy/=ln2  \\\\
Bisher ist die Implementierung zum Berechnen von Entropie schon fertig. Aber wenn man einen R\"uckblick auf den Code wirft, ist es nicht so schwer zu entdecken, dass daher eine Optimierung mit SIMD-Befehlen m\"oglich ist. Wir konvertieren dann auch Schritt f\"ur Schritt die Implementierung mit SIMD-Befehlen:\\\\
 {\Large 1.}Versuchen, die Implementierung f\"ur \"Uberpr\"ufung auf Eingaben zu konvertieren:\\
Das Aufsummieren von Elementen im Array kann man nat\"urlich mit SIMD-Befehlen verbessern, aber diese Verbesserung finden wir als entbehrlich. Weiter finden wir eine Reform f\"ur den Test auf $0.0f < data[i] ≤ 1.0f$ mit SIMD-Befehlen unrealisierbar. Deshalb machen wir keine weiteren Optimierungen f\"ur \"Uberpr\"ufung auf Eingaben.\\\\
{\Large 2.}	Versuchen, die Implementierung zum Berechnen f\"ur $ln{X}$ zu konvertieren:\\
Dieser Teil ist eigentlich nicht so schwer. Hier ist es aber wichtig zu beachten, dass abgesehen von der SIMD-Optimierung wir noch eine Ver\"anderung gemacht: die Endergebnisse, die in q0 gespeichert werden, sind nicht einfach die Werte von $ln{(data[i])}$ f\"ur vier floats, sondern die Werte von$ data[i] * ln(data[i])$. Dar\"uber hinaus muss man darauf aufpassen, dass der Befehl vdiv nicht direkt f\"ur q Register verwendet werden kann. ({\color{red}Trick 2: Statt $\displaystyle{\frac{X^n}{n}}$ mit vdiv zu berechnen, soll man zuerst $\displaystyle{\frac{1}{n}}$ rechnen, und dann das Ergebnis mit vdup zu duplizieren, zuletzt den Befehl vmul f\"ur $\displaystyle{\frac{1}{n}}$ und $X^n$ verwenden.})\\
\\\\
{\Large 3.}	Versuchen, die Implementierung zum Berechnen f\"ur Entropie zu konvertieren:
Dieser Teil ist auch nicht schwierig. Der einzige Punkt, auf den man achten soll, ist, dass man in der Schleife f\"ur jede Runde \"iberpr\"ufen soll, ob es noch mehr als vier Elemente im Array gibt. Das kann man aber durch einen Vergleich zu der L\"ange von dem Array gut realisieren. Wenn die Anzahl von verbleibenden Elementen im Array kleiner als 4 ist, muss man auf simd\_lnX verzichten und auf den alten Code $ln{X}$ springen.\\ 

\section{Bonus Aufgabe}
%{\Huge IV)} \\
\subsection{Random-Input}
\begin{large}
%{\LARGE 1)Random-Input }\\

Um ein Array mit einem grossen Length von zuf\"alligen Wahrscheinlichkeiten, deren Summe 1 ist , zu erzeugen:\\
-	Wir erzeugen ein Array ( Main Array ) mit einem grossen Length ( zB 30000)  \\
-	Alle Elemente von Main Array sind Zahlen zwischen [0..RAND\_MAX] \\
-	Mit Hilfe einem Sortierungsverfahren, wir erzeugen ein anderes Array ( Second Array ) , das die gleichen Elemente vom Main Array aber ohne Wiederholungen enth\"alt, und ein anderes Array mit dem gleichen Length (Third Array ) , das die Anzahl von Wiederholungen jedes Element vom Second Array enth\"alt.\\
-	Um die Wahrscheinlichkeiten zu bestimmen, wir erzeugen ein neues Array (Probability Array) mit dem gleichen Length und dessen Elemente $\frac{Anzah von Widerholungen}{length von main Array}$ sind.\\
{\color{red}Beispiel:} \\

{\large -Main Array :} \\
\begin{Large}


\begin{tabular}{l|l|l|l|l|l|l|l}
\hline 
1 & 5 & 2 & 1 & 6 & 2 & 4 & 6 \\

\end{tabular}
\end{Large}

{\large -Second Array (ohne Wiederholung): }\\
\begin{Large}


\begin{tabular}{l|l|l|l|l}
\hline 
1 & 2 & 4 & 5 & 6\\

\end{tabular}
\end{Large}

{\large -Third Array (Anzahl von Wiederholungen): } \\
\begin{Large}


\begin{tabular}{l|l|l|l|l}
\hline 
2 & 2 & 1 & 1 & 2\\

\end{tabular}
\end{Large}

{\large -Probability Array:} \\
\begin{Large}


\begin{tabular}{l|l|l|l|l}
\hline 
0.25 & 0.25 & 0.125 & 0.125 & 0.25\\

\end{tabular}
 \\\\
\end{Large}






\end{large}

\begin{large}
\subsection{calc\_entropy\_approx Methode:}
%{\LARGE 2)calc\_entropy\_approx Methode: } \\

Bisher ist der praktische Teil f\"ur unser Projekt schon fertig.Wir machen dann die zweite Bonus Aufgabe: eine andere Methode calc\_entropy\_approx zu implementieren, wobei man eine Ann\"aherung zu $log_2{X}$ lediglich mithilfe von Shifts (z.B. lsl, lsr, usw.) simulieren muss. Daher ist ein besonderes Thema betroffen: wie eine Fliesskommazahl im Rechner gespeichert wird (Theoretischer Teil 3.).\\\\
Das Problem besteht darin, wie man die Mantisse-Teil von $log_2{X}$ bekommen kann. Unsere Idee hier ist eine Interpolation ({\color{red}hier unten erkl\"art}). \\\\
{\color{red}Interpolieren zwischen zwei ganzen Zahlen} \\\\
\"Uber dieses Thema haben wir eine Arbeit gefunden, und das Diagramm in dieser Arbeit ist ziemlich bedeutungsvoll (http://degiorgi.math.hr/aaa\_sem/Div/97-105.pdf ). \\\\
\includegraphics[scale=0.8]{image}\\
Die Idee daher ist, eine Interpolation durch Gerade zu simulieren. Zum Beispiel: \\\\
	$log_2{150 } \approx 7.2288$ \\
    $log_2{170} \approx 7.4094$ \\\\
Die letzte 2er Potenz (kleiner als 150 und 170), der ein ganzzahliges Ergebnis von $log_2$ hat, ist 128, da $ 128 = 2^7$ \\\\
Die n\"achste 2er Potenz (gr\"osser als 150 und 170), der ein ganzzahliges Ergebnis von $log_2$  hat,ist 256, da $256 = 2^8$ \\\\
Die Distanz zwischen 256 und 128 ist$ = 256 - 128 = 128 = 2^7$. Die Distanz ist gleich die letzte 2er Potenz. Das ist aber kein Zufall, da $2^{n+1} - 2^n = 2^n*(2 - 1) = 2^n$ \\\\
Das Interpolieren sieht so aus:\\\\
	$ \displaystyle{  log_2{150} \approx 7 + \frac{Distanz zwischen \ 150 \ und \ 128}{Distanz zwischen \ 256 \ und \ 128}} $ \\\\
	(oder $ \displaystyle{ \approx 8 - \frac{Distanz zwischen \ 256 \ und \ 150} {Distanz zwischen \ 256  \ und \ 128}}$) \\\\
Zum Berechnen: \\\\
 $log_2{150} \approx 7 + \frac{22}{128} = 7.171875$\\	
(oder $8 - \frac{106}{128} = 7.171875$)\\\\
Analog dazu gilt:\\\\
	$log_2{170} \approx 7.328125$\\\\
F\"ur die Werte, die kleiner als 1.0f sind, ist der Prozess aber gleich.\\\\
Wir brauchen jetzt nur das Interpolieren zu implementieren. Und w\"ahrend der Implementierung haben wir noch weiteren Trick gefunden. ({\color{red} Trick 3})\\
{\color{red} Trick 3:} \\
Angenommen, dass ein float f ist:\\
$$ 1.abcdef * 2^n$$
Die letzte 2er Potenz, die kleiner gleich als f ist, ist:
	$$2^n$$
Die n\"achste 2er Potenz, die gr\"osser gleich als f ist, ist:
	$$2^{n+1}$$
Die Distanz dazwischen ist:
	$$2^{n+1} - 2^n = 2^n*(2 - 1) = 2^n$$
Die Distanz zwischen f und der letzten 2er Potenz 2n ist:
$$1.abcdef * 2^n - 2^n = 2^n (1.abcdef - 1)$$
Das Interpolieren ist dann:
	$$n + \frac{2^n*(1.abcdef - 1)}{2^n} = n + (1.abcdef - 1)$$
Der Wert 1.abcdef ist aber super einfach zu „erzeugen“: Man braucht nur den Exponent-Teil von f auf 127 setzen, da das Exponent von $1.abcdef = 1.abcdef * 2^0$ 0 ist, und $0 + 127 = 127$. Und danach kann man sehr leicht $1.abcdef – 1$ rechnen.\\

\section{Performanz und Genauigkeit:}
Bisher ist die Implementierung schon fertig. Wir fangen jetzt an, auf die Performanz und die Genauigkeit zu pr\"ufen. 
\subsection{Pr\"ufung auf Performanz :}
Beim Testen auf Performanz werden die Laufzeiten von calc\_entropy, calc\_entropy\_old und calc\_entropy\_c2 verglichen. Folgende Situationen werden hier gepr\"uft:\\
{\large 1.}	Die Laufzeiten f\"ur 50000 Mal Ausf\"uhren\\
{\large 2.}	Die Laufzeiten f\"ur 100000 Mal Ausf\"uhren\\
{\large 3.}	Die Laufzeiten f\"ur 200000 Mal Ausf\"uhren\\
{\large 4.}	Die Laufzeiten f\"ur 500000 Mal Ausf\"uhren\\\\
{\large 1.}	Die Laufzeiten f\"ur 50000 Mal Ausf\"uhren mit 16-Elemente Array\\
\centerline {\includegraphics[scale=0.8]{6}}
Betrachten: $$ \frac{7.861029}{3.119230} \approx 2.52$$\\
                     $$ 7.138445 < 7.861029$$\\\\
2.	Die Laufzeiten f\"ur 100000 Mal Ausf\"uhren mit 16-Elemente Array\\

\centerline {\includegraphics[scale=0.8]{7} }
Betrachten: $$\frac{15.722114}{6.099190}≈ 2.58$$\\
                      $$14.276830 < 15.722114$$\\

3.	Die Laufzeiten f\"ur 200000 Mal Ausf\"uhren mit 16-Elemente Array\\

\centerline {\includegraphics[scale=0.8]{8} }
Betrachten: $$\frac{31.444210}{12.044804} \approx 2.61 $$\\
                     $$ 28.553687 < 31.444210$$\\

4.	Die Laufzeiten f\"ur 500000 Mal Ausf\"uhren mit 16-Elemente Array\\
\centerline  {\includegraphics[scale=0.8]{9} }
Betrachten: $$\frac{78.609377}{30.106788}\approx 2.61$$\\
                     $$ 71.384321 < 78.609377$$\\\\
Man kann hier Punkte bemerken:\\
{\Large 1.}	calc\_entropy (mit SIMD-Befehlen) ist wie erwartet schneller als calc\_entropy\_old (ohne SIMD-Befehle), aber nicht 4-fach schneller (q Register kann 4 floats speichern und damit Multiplikation, Addition, Subtraktion gleichzeitig f\"ur 4 floats zu einem anderen q Register behandeln), sondern nur ungef\"ahr 2.6-fach. Nach unserer Vermutung gibt es folgende Gr\"unde daf\"ur:\\
erste: f\"ur die \"Uberpr\"ufung auf Eingabevalidit\"at gibt es keine SIMD-Optimierung
zweite: die Laufzeit f\"ur Multiplikation (vmul) ist im Vergleich zu Addition (vadd) und 
              Subtraktion (vsub) ziemlich stark abh\"angig von Multiplikatoren und auch von 
              Registertypen (s, d, q)\\
{\Large 2.	}calc\_entropy\_c2 (implementiert Reihendarstellung in C) ist immer schneller als calc\_entropy\_old (implementiert Reihendarstellung in Assembler).\\\\
\subsection{Pr\"ufung auf Genauigkeit:}
Am Ende f\"uhren wir die \"ufung auf Genauigkeit. Hierbei werden die Berechnungsergebnisse von calc\_entropy und von calc\_entropy\_approx mit Standard-Ergebnis von calc\_entropy\_c1 verglichen. Da wir interessieren uns nur auf Berechnungsergebnis von $log_2{X}$, haben wir den Code ein bisschen ver\"andert. (z.B. keine \"Uberprufung auf Eingaben, Array hat immer nur 1 Element, R\"uckgabewert ist nicht mehr Entropie sondern einfach $log_2{X}$ …). Beim ersten Test haben wir 14 Eingaben (von 0.00001f bis 1.0f, aufsteigend sortiert). Wir stellen die Ergebnisse mithilfe von Tabelle dar:\\\\
\includegraphics[scale=0.8]{image4}\\\\
Beim zweiten Test haben wir 10 Eingaben (von 1.0f bis 0.015625f, absteigend sortiert, alle Eingaben sind 2er Potenz). Wir fassen die Daten wie oben in Form von Tabelle zusammen:\\\\
\includegraphics[scale=0.8]{image5}\\\\
Nachdem man die Daten analysiert, ist es nicht schwer zu entdecken, dass:\\
1. Der Approximationsfehler von calc\_entropy wird gr\"osser, wenn Inputwert klein wird. Wenn der Inputwert ziemlich klein ist, z.B. 0.00001f, ist der Fehler auffallend gross.\\\\
2. Der Approximationsfehler von calc\_entropy\_approx wird gr\"osser, wenn Inputwert grosser wird. (abgesehen davon, dass der Inputwert 2er Potenz ist)\\\\
3. calc\_entropy\_approx hat immer kleine Ergebnisse (gr\"ossere Betr\"age) im Vergleich zu calc\_entropy. Das ist aber schon wie vorgestellt klar, weil die Funktion $log_2{(X)}$ konkav ist. Das f\"uhrt dazu, dass das Interpolieren mithilfe von Gerade zwischen zwei benachbarten 2er Potenzen immer unterhalb der Kurve von $log_2{(X)}$ bleibt.\\\\
4. calc\_entropy\_approx hat gar keinen Approximationsfehler, wenn der Inputwert gerade eine 2er Potenz ist. Das ist auch wie vorher vorgestellt klar, weil in diesem Fall man keine Interpolation braucht.\\






\end{large}

















 



\end{document}
